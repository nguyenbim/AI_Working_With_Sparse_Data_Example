# -*- coding: utf-8 -*-
"""Working_with_sparse_gene_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RESuWrf9AQrhdBxRNhgy14QdIMIb1fJq
"""

# Commented out IPython magic to ensure Python compatibility.
# import các thư viện
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
from sklearn.metrics.pairwise import euclidean_distances
# %matplotlib inline
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from tqdm import tqdm

from sklearn.model_selection import learning_curve
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
# %matplotlib inline
random_state = 42

lines=[]
set_feature=['0,0', '1,1', '1,2', '2,2']
map_feaute={
    v:i for i,v in enumerate(set_feature)
}
from tqdm import tqdm
with open('Training_data.csv',"r") as f:
  for line in tqdm(f.readlines()):
    line=line.strip()
    line=line.split(",")
    feature=[]
    for k in range(0,len(line),2):
      feature.append(map_feaute[f"{line[k]},{line[k+1]}"])
      # set_feature.add(feature[-1])
    lines.append(feature)

df=pd.DataFrame(lines)
df.head()

# code 
df_label = pd.read_csv('Training_label.csv')
df_label.head()

y = df_label['Label']
y

y.value_counts()

# set all 0 in df as nan
df.replace(0, np.nan, inplace=True)
df.head()

df1 = df.dropna(axis=1)
df1.head()

lines=[]
set_feature=['0,0', '1,1', '1,2', '2,2']
map_feaute={
    v:i for i,v in enumerate(set_feature)
}
from tqdm import tqdm
with open('Validation_data.csv',"r") as f:
  for line in tqdm(f.readlines()):
    line=line.strip()
    line=line.split(",")
    feature=[]
    for k in range(0,len(line),2):
      feature.append(map_feaute[f"{line[k]},{line[k+1]}"])
      # set_feature.add(feature[-1])
    lines.append(feature)

df_test = pd.DataFrame(lines)

df_test.head()

#list columns of df1
df1.columns

df_test1 = df_test.copy()
df_test1 = df_test1.drop(columns=[col for col in df if col not in df1.columns])
df_test1.head()

# set values in Y for PTC is 0 and Control is 1
y[y == 'PTC'] = 1
y[y == 'Control'] = 0
y.value_counts()

y.value_counts()
y=y.astype(int)

df1

"""### Test drop all Null values and RandomForest default"""

# add random forest classifier, knn classifier, linear regression, ridge regression, lasso regression to list model
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

rf = KNeighborsClassifier()
rf.fit(df1, y)
Y_pred = rf.predict(df_test1)

print(Y_pred)

"""### Test using list highest entropy or IG"""

# calcalate Entropy and information gain of each feature for label PTC and Control
from sklearn.metrics import mutual_info_score

# calculate for each feature of df1 and get list of df1 information gain with feature name
list_info_gain = []
for i in df1.columns:
    info_gain = mutual_info_score(y, df1[i])
    list_info_gain.append(info_gain)

list_info_gain

# get 1000 highest information gain feature and get list of df1 information gain with feature name
list_info_gain = sorted(zip(list_info_gain, df1.columns), reverse=True)[:1000]
list_info_gain

# new df from df1 with column from 1000 highest information gain feature
df_new = df1[list_info_gain[0][1]]
for i in range(1,1000):
    df_new = pd.concat([df_new
    , df1[list_info_gain[i][1]]], axis=1)
df_new.head()

df_new_test = df_test1[list_info_gain[0][1]]
for i in range(1,1000):
    df_new_test = pd.concat([df_new_test, df_test1[list_info_gain[i][1]]], axis=1)
df_new_test.head()

rf = KNeighborsClassifier()
rf.fit(df_new, y)
Y_pred = rf.predict(df_new_test)

Y_pred

# set y as a column of df1
df2 =df1.copy()
df2['Label'] = y
df2

# df3 is df2 with all rows with Label == 0
df3 = df2[df2['Label'] == 0]
df3

from sklearn.model_selection import cross_val_score
rf=RandomForestClassifier()
cross_val_score(rf, df_new, y,scoring='f1', cv=10).mean()

df_new

df_new_test

y

rf.fit(df_new, y)
y_pred = rf.predict(df_new_test)

y_pred

"""###  => features work for Control not PCT

### Try collect features with highest IG for PCT
"""

from operator import index


IG_columns =[]
# for each column in df2
for i in df2.columns:
    # if column is not Label
    if i != 'Label':
        # collect number of all type of value in column
        df2[i].value_counts()
        # collect number of label == 0 for each type of value in column
        df2[i][df2['Label'] == 1].value_counts()
        temp =0
        for j in df2[i].value_counts().index:
            if j in df2[i][df2['Label'] == 1].value_counts().index:
                temp += (df2[i][df2['Label'] == 1].value_counts()[j]/115) * np.log2(df2[i][df2['Label'] == 1].value_counts()[j]/df2[i].value_counts()[j])
            else:
                temp += 0
        IG_columns.append(temp)
print(IG_columns)

len(IG_columns)

# get 5 lowest IG_columns and get list of df1 with feature name
list_IG_columns = sorted(zip(IG_columns, df1.columns), reverse=True)[:5]
print(len(list_IG_columns))
list_IG_columns

df_new2 = df1[list_IG_columns[0][1]]
for i in range(1,5):
    df_new2 = pd.concat([df_new2
    , df1[list_IG_columns[i][1]]], axis=1)
df_new2

df_new_test2 = df_test1[list_IG_columns[0][1]]
for i in range(1,5):
    df_new_test2 = pd.concat([df_new_test2, df_test1[list_IG_columns[i][1]]], axis=1)
df_new_test2

rf=RandomForestClassifier()
cross_val_score(rf, df_new2, y,scoring='f1', cv=10).mean()

rf=RandomForestClassifier(n_estimators=500, max_depth=1000)
rf.fit(df_new2, y)
y_pred = rf.predict(df_new_test2)
print(y_pred)

y_test = np.array([1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])
len(y_test)

# f1 score y_pred vs y_test
from sklearn.metrics import f1_score
f1_score(y_pred, y_test)

df2

from operator import index


IG_columns2 =[]
# for each column in df2
for i in df2.columns:
    # if column is not Label
    if i != 'Label':
        # collect number of all type of value in column
        df2[i].value_counts()
        # collect number of label == 0 for each type of value in column
        df2[i][df2['Label'] == 0].value_counts()
        temp =0
        for j in df2[i].value_counts().index:
            if j in df2[i][df2['Label'] == 0].value_counts().index:
                temp += (df2[i][df2['Label'] == 0].value_counts()[j]/115) * np.log2(df2[i][df2['Label'] == 0].value_counts()[j]/df2[i].value_counts()[j])
            else:
                temp += 0
        IG_columns2.append(temp)
# print(IG_columns)

IG_columns2

"""### Try K-means to collect set of features for PCT classification"""

df_T1 = df1.transpose()
df_T1.head()

from sklearn.cluster import KMeans
km = KMeans(n_clusters=20)
km.fit(df_T1)

# divide df_T1 into 20 clusters
df_T1['cluster'] = km.labels_
df_T1.head()

# create x_train for each cluster 
x_train = []
for i in range(20):
    x_train.append((df_T1[df_T1['cluster'] == i]).drop(columns=['cluster']).transpose())
x_train

x_train[1]

# run cross validation for each x_train using rf
from sklearn.model_selection import cross_val_score
cross_grades = []
for i in range(20):
    rf=RandomForestClassifier()
    cross_grades.append(cross_val_score(rf, x_train[i], y,scoring='f1').mean())
cross_grades

